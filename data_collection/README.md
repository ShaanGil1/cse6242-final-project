# data_retrieval.ipynb
### Description
This notebook pulls data from a github repo and compiles the data and filters by if location data is present.  It then uses the compiled data to make batched twitter API calls to retrieve the relevant fields for this project and saves all this data into a file. 

### Installation
Requirements: \
python==3.7 \
jupyter==1.0.0 \
pandas==1.1.0 \
tweetpy==(most recent version)

### Execution
Run all the import cells to make sure you have the all needed packages. And copy paste personal twitter keys into appropriate cell block.\
Clone the following github repo:\
https://github.com/shaypal5/awesome-twitter-data

Select the time frame of data that needs to be gather by deleting data that doesn't need to be gathered. The first half the file will compile the data from the github repo and the second half will use the tweet ids to retrive all the data.  The following 4 paths need to be adjusted on the notebook to match the machine that is running the file:\
-Root path of all the data that is in being collect (data pulled from github repo)\
-Path of where compiled data will be stored\
-Path of compiled data (same as one above)\
-Path of where all twitter data will be stored

This file will pull the fields that we found relavant to our project however they can be adjusted if other fields need to be pulled.

# emotional_analysis.ipynb
### Description
This notebook takes in twitter data and gives analyzes each tweet and gives an emotion score to each tweet.  It complies the results and outputs the dataset to a path designated by the user.

### Installation
Requirements: \
python==3.7 \
jupyter==1.0.0 \
pandas==1.1.0\
nltk==3.7\ 
emoji==1.6.3 \
text2emotion == 0.0.5

### Execution
The file is structured so that it takes in batches of tweets at a time adjust the batch number to match what you did (or run it all at once). Run all of the cell and make sure all of the import requirements are met.  The file will run and on the last cell block provide a file path that the output will be stored in.

# Data_cleanup.ipynb
### Description
This notebook is used to generate the .csv files necessary for our visualizations as well as some miscellanaeous files and data that was used for the experiments of the report.

### Installation
Requirements: \
python==3.7 \
jupyter==1.0.0 \
numpy==1.19.1 \
pandas==1.1.0 

### Execution
All sections (marked by v# in the first cell) can be run by running the import cell at the top then running the cells 1 by 1, assuming you have the necessary .csv file from the cdc website:\
https://data.cdc.gov/Vaccinations/COVID-19-Vaccinations-in-the-United-States-Jurisdi/unsk-b7fc/data 

our google drive:\
https://drive.google.com/drive/folders/1jmqrrgiNyqUFemB4LCvO1LK7Uyl2nnd-?usp=share_link 
or generated by an earlier section. 
Paths should function in a way that's system agnostic if you place everything in the data_collection folder.  

# nltk_sentiment
### Description
This notebook is used to take the cleaned data and calculate the sentiment of the text using the python nltk library. Produces a single csv with added collumns for sentiment scores

### Installation
Requirements: \
python==3.7 \
jupyter==1.0.0 \
nltk==3.7

### Execution
Run the cells in order. Use the cleaned dataset output from data_cleanup.ipynb

# getMostInfluential.py
### Description
This python script creates a database from the csv produced by nltk_sentiment notebook. It then performs sql operations to retrieve the users that make the most covid-19 related tweets as well as users that have the most likes on covid related tweets. The output includes the twitter user's id. The user's username can be obtained through twitter's api or through the website https://tweeterid.com/

### Installation
Requirements: \
python==3.7 \
jupyter==1.0.0 \
sqlite==3.11

### Execution
Run the script then use the output to convert the user's id to their username.


